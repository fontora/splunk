https://conf.splunk.com/files/2020/slides/PLA1302C.pdf
Performance
• Volume of Ingest
• Search Performance
• More Users
• Big Apps
• Real-Time

Capacity
• Volume of Ingest
• Index Retention Periods
• Indexer Clustering
• Big Apps


CPU, MHz not just #
IOPS, 1500-2000 per IDX
RAID 10
NVMe

| 7.1 | Parallel reduce search processing (new command: `redistribute`) |

Cascading bundle replication in 8
https://conf.splunk.com/files/2019/slides/FN2514.pdf

data distribution
EventBreaker
pipelines
gr8
kv_mode, tz, annotate punct
index extractions

tsdix version
journalcompression

search concurrency
search length

how many indexes

workload management?

| eval event_delay=_indextime - _time


EVENT DISTRIBUTION!
https://conf.splunk.com/files/2019/slides/FN1402.pdf
https://github.com/silkyrich/cluster_health_tools

indexer discovery

Indexed realtime: https://docs.splunk.com/Documentation/Splunk/8.1.2/Search/Aboutrealtimesearches#Indexed_real-time_search
remote_timeline_fetchall: https://docs.splunk.com/Documentation/Splunk/8.1.2/DistSearch/Troubleshootdistributedsearch#Network_problems_can_reduce_search_performance

allow_skew

Configs to set
https://conf.splunk.com/files/2019/slides/FN1635.pdf
https://conf.splunk.com/files/2019/slides/SEC1554.pdf

TSTATS and PREFIX
https://conf.splunk.com/files/2020/slides/PLA1089C.pdf


pstacks(eu-stack

New KV store

ubuntu 18?

22nd feb
We've done a bit of housekeeping on our end and we've been changing the
cron schedule on some aggressive scheduled searches.
Any of the searches set to a 1 minute cron have been changed to a 30 minute
schedule. We're continuing to look at some of the expensive queries too in
order to work out how we can better optimise them.

There has been ongoing battle on indexing queue busy issue on their indexer layer in which I believe they have implemented WLM(Workload Management) in a way that searches are not hogging all available CPUs on the indexer layer. @Jeremy Chen worked on this issue previously.
This is also one of the reasons why they have throttled down search capacity(scheduled searches + adhoc searches) in half of their full search capacity on purpose.  And I remember on a Zoom session that they mentioned they can increase their indexer's capacity if necessary.    If the issue is not from Splunk scheduler's issue, it's high time for them to consider scaling up vertically or horizontally instead of throttling down search capacity.

### MIGRATION SEARCHES ###

index=customer_splunkd Cachemanager TERM("action=upload") sourcetype=splunkd TERM("status=succeeded") kb>1000 stack=<my-stack> host=idx-i-*.<my-stack>.splunkcloud.com
| eval speed_kbs=round(kb/elapsed_ms*1000,0) 
| rex field=cache_id "(?<b_type>[\w]+?)\|(?<idx>[\w\-_]+?)~" 
|  table _time host kb elapsed_ms speed_kbs cache_id idx b_type

| timechart avg(speed_kbs)  by host” or sum(kb), sum(elapsed_ms) and do a timechart with span=1s to see parallel uploads



tsidxwritinglevel  journal-compression  journal-size  tsidx size
      1                     gzip            156MB       258MB
      2                     gzip            156MB       217MB
      3                     zstd            149MB       222MB!!!!!!!
      3                     lz4             237MB       221MB!!!!!!!

aaa_json1: Level1, GZIP, journal.gz =76.002.583, TSIDX=104.375.704
aaa_json5: Level3, ZSTD, journal.zst=74.608.439, TSIDX= 86.932.905      15% saving
aaa_json7: Level4, ZSTD, journal.zst=74.608.485, TSIDX= 60.839.147      40% saving


######################


lru		vs		lruk 

lruk combines with frequency of access... so a blended approach.
- suggested

hot data
- 7 days?


search role, how far back in time you can search
==> protection to stop people pulling back EVERYTHING from s3


######################

Health Check            40 credits, 4 days
- WLM
- Indexing pipeline review
- SHC review
- Search profile
- CPU profiling         pstacks, get support to help do cpu profiling

CHAT WITH CHANDRA 
 
=====> 

##### Chat about the move to tsidx3 and 750mb buckets

Avalon, Bondi, Coogee, Deewhy (Jira)

stream processing... 1m batches, or 2m


Change algorithm ~5 months ago.

James: How do we protect the indexing service during changes

50 - 70 indexers, 20 new ones
In server.conf [clustering] on Indexer:
[clustering]
cxn_timeout = 600
send_timeout = 600
rcv_timeout = 600
heartbeat_period = 10
#In server.conf [clustering] on Cluster Master:
[clustering]
executor_workers = 16
heartbeat_timeout = 600
cxn_timeout=600
send_timeout=600
rcv_timeout=600
max_peer_build_load= 5
max_fixup_time_ms = 5000
max_peers_to_download_bundle = 5
#Indexer Side (ALL INDEXERS AND CLUSTER MASTER)
In server.conf
[httpServer]
busyKeepAliveIdleTimeout = 180
streamInWriteTimeout = 30



WLM (index=*? you go to sin-bin and run in 1% of total CPU available)



look for symptoms
-- indexer health ===> good starting block

1% top 80-90%
 
 pipelines / events per squirt


##1 Discovery searches
## Debug ingestion
## Indexing performance over time

Find IP Addresses ===> index fields for IP addresses

cost per search
cost per ingestion




| dbinspect 
| rest /services/data/indexes 
index=_internal
index=_introspection
